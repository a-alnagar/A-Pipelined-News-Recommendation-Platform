{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Updates Cell**\n",
        "\n",
        "*18th May, 2022*\n",
        "\n",
        "This model was chosen since it was pretrained on news summarization datasets (CNN & Dailymail).\n",
        "\n",
        "The BART model only accepts tokens with size of 512 (it was pretrained on this size). We can divide each article to batches of 512 tokens and concantenate them later. To adjust tokens size to 2048 the model will need to be retrained.\n",
        "\n"
      ],
      "metadata": {
        "id": "3de8V-_ypbui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vApeDLqMDevc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LiB4aY6jPtsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "v5BVRFyuRbBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from transformers import BartForConditionalGeneration, BartConfig, BartTokenizer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "C2SRP5ue_ySo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/samples')\n",
        "news = df.copy()\n",
        "news.columns = ['sk', 'url', 'title', 'text', 'tags', 'count', 'date', 'summary']\n",
        "test = news.loc[1:5]\n",
        "\n",
        "#convert the articles column to a list for summarization\n",
        "#this was done to avoid using apply as it has a big running time\n",
        "\n",
        "articles = test['text'].tolist()\n",
        "nums = test['count'].tolist()\n",
        "news = news.sort_values(by = ['count'])"
      ],
      "metadata": {
        "id": "YRclCIsYVVWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Intilizing the BART model \n",
        "model =  BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "#intializing text tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "\n",
        "#The following piece of code summarizes the articles\n",
        "#NOTE: the model accepts a maximum length of 512 words, so large articles\n",
        "#are summarized iterativley\n",
        "\n",
        "sums = []\n",
        "temp = ''\n",
        "num = 0\n",
        "count = 0\n",
        "for article in articles:\n",
        "  if nums[num] <= 100:\n",
        "    sums.append(article)\n",
        "    num += 1\n",
        "  \n",
        "  else:\n",
        "    while(nums[num] > 10):\n",
        "        inputs = tokenizer([article[count: count +  512]], max_length = 512, return_tensors=\"pt\")\n",
        "        summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=50)\n",
        "        temp +=  tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
        "        nums[num] -= 512\n",
        "        count += 512\n",
        "    print(temp + '\\n')\n",
        "    num += 1\n",
        "    count = 0\n",
        "    sums.append(temp)\n",
        "    temp = ''\n",
        "\n",
        "\n",
        "\n",
        "newspaper = {'nytimes':'new york times', 'washingtonpost':'washington post', 'theguardian':'the guardian'}\n",
        "\n",
        "#This function creates a sperate column contains the website name\n",
        "def paper(df):\n",
        "  col = df['url'].lower()\n",
        "  for k in newspaper.keys():\n",
        "    n = re.search(k, col)\n",
        "    if n != None:\n",
        "      return re.sub(col,newspaper[k], col)\n",
        "\n",
        "#This fucntion cleans the title from non-alphanumeric characters and removes\n",
        "#the website name from the title\n",
        "def cleaner(df):\n",
        "  col = df['title'].lower()\n",
        "  for k in newspaper:\n",
        "    if k in col:\n",
        "      col = col - k\n",
        "  return ''.join(char for char in col if char.isalnum() or char == ' ')\n",
        "\n",
        "\n",
        "def text_cleaner(df):\n",
        "  c = df['text'].lower()\n",
        "  #remove words between brackets\n",
        "  c = re.sub(\"[\\(\\[].*?[\\)\\]]\", '', c)\n",
        "  #remove all special characters and numbers from main text\n",
        "  return str(''.join(char for char in c if char.isalpha() or char == ' '))\n",
        "\n",
        "\n",
        "#Create a paper column\n",
        "news['paper'] = news.apply(paper, axis = 1)\n",
        "\n",
        "#Clean title column\n",
        "news['title_new'] = news.apply(cleaner, axis = 1)\n",
        "news['title'] = news['title_new']\n",
        "news = news.drop(['title_new'], axis = 1)\n",
        "\n",
        "print(test.head(10))\n"
      ],
      "metadata": {
        "id": "4mVpmb8tD022"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "There are sum patterns in the summarized text needs removal:\n",
        "1. ''\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OZ6qkXcu4Cp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "New Recommender Pre-processing",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}